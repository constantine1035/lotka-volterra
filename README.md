# Отчёт по внедрению и сравнительному анализу эволюционных GAN-моделей на CelebA

---

## 1. Цели и мотивация

Основная цель проекта — исследовать и реализовать популяционные (эволюционные) обобщения классических Generative Adversarial Networks (GAN), предложенные в статье «Lotka–Volterra Model with Mutations and Generative Adversarial Networks» (Kozyrev, 2024). Ключевые задачи:

1. **Реализовать модель Лотка–Вольтерра с мутациями** (LVM–GAN) в дискретном виде на ансамбле сетей (эволюционная стратегия, ES).  
2. **Понять причины нестабильности** чистой эволюционной GAN (появление шума вместо лиц даже спустя сотни «поколений»).  
3. **Добавить градиентные шаги** (гибрид ES+SGD) для стабилизации и ускорения сходимости.  
4. **Модифицировать архитектуры** под сверточный GAN (DCGAN, WGAN) и повторить эксперименты.  
5. **Сравнить** финальный гибрид с классическим GAN-базлайном.  

---

## 2. Краткий обзор предложенной модели

В статье вводится система PDE для плотностей популяций дискриминаторов \(f(y,t)\) и генераторов \(g(z,t)\):
\[
\frac{\partial f}{\partial t}
=M_d\Delta f + A(y)f - N_d\,f\int B(y,z)\,g\,dz,\quad
\frac{\partial g}{\partial t}
=M_g\Delta g - C\,g + N_g\,g\int B(y,z)\,f\,dy.
\]
- **\(\Delta\)** — лапласиан Белтрами на статистическом многообразии (мутации).  
- **\(A(y)\)** — «фитнес» дискриминатора (распознавание реальных данных).  
- **\(B(y,z)\)** — «хищнический» член (генератор «поедает» узкие пики дискриминатора).  
- Решение этих уравнений даёт pop-based GAN, естественно контролирующий переобучение за счёт подавления узких пиков плотности .

В чистой непрерывной форме PDE не применимы для обучения высокоразмерных сетей. Поэтому мы перешли к дискретным популяционным стратегиям.

---

## 3. Реализованные модели и подходы

### 3.1. Чистая эволюционная стратегия (ES)  
- **Популяция**: \(N_d\) дискриминаторов и \(N_g\) генераторов (на практике \(N=10\)).  
- **Фитнес** \(A_i = \mathbb{E}_{x\sim\text{real}}\ln D_i(x)\),  
  \(\;B_{ij} = -\mathbb{E}_{z}\ln(1-D_i(G_j(z)))\).  
- **Отбор** жёсткий: лучшие 5 из 10, клонирование и мутации (гауссов шум σ≈0.02).  
- **Результат**: даже к 200–му поколению — чистой шум .

### 3.2. Сверточные DCGAN-архитектуры в ES  
- Заменили полносвязные сети на **ConvTranspose2d / Conv2d** (DCGAN-подобие).  
- Размер изображений **32×32** (ресайз CelebA).  
- **Результат**: шум сохранялся, т.к. всё ещё не было локального градиентного сигнала.

### 3.3. WGAN + ES  
- Перешли на **WGAN-архитектуру** (критик без сигмоида, клиппинг весов ±0.01).  
- Популяционный ES с фитнесом \(E[D(\text{real})]-E[D(\text{fake})]\) и мутациями генераторов.  
- **Результат**: снова шум, малоинформативный сигнал от критика без градиента.

### 3.4. Гибрид ES + градиент (ES–SGD)  
- **Шаг 1** (ES): отбор и мутации популяций D и G (WGAN).  
- **Шаг 2** (градиентная фаза): для каждого индивидуума — 5 шагов RMSprop критика + 1 шаг генератора.  
- **Плюс**: локальный градиентный сигнал + глобальное разнообразие ES.  
- **Результат**: появилась структура на лицах, но обучение было громоздким и тяжёлым по ресурсам.

### 3.5. Гибрид DCGAN + эволюция (оптимальный вариант)  
- **Один дискриминатор** обучается классическим Adam + BCE на батче:  
  \(\mathcal{L}_D = -\bigl[\log D(x_{\rm real}) + \tfrac{1}{N}\sum_{i}\log(1-D(G_i(z)))\bigr].\)
- **Популяция 5 генераторов**: каждый делает по одному Adam–шагу («обмануть D») в рамках одной эпохи.  
- **Эпоха** = полное прохождение даталоадера + эволюционный отбор по среднему \(\mathcal{L}_G\).  
- **Селекция**: оставляем 2 лучших G, клонируем и мутируем остальных.  
- **Визуализация**: вывод сетки из 16 изображений раз в 10 эпох.  
- **Результаты**:  
  - Ужe к 10-й эпохе распознаваемые лица .  
  - Быстрая и стабильная сходимость по сравнению с чистым GAN и чистым ES.

### 3.6. Vanilla DCGAN-базлайн  
- Одна пара G / D, классический цикл Adam+BCELoss.  
- Среднее качество лиц после 10–20 эпох заметно уступало гибриду (лицы «размыты», менее чёткие детали).

## 4. Технические детали реализации

- **Датасет**: CelebA, ресайз до 32×32, нормализация \((-1,1)\).  
- **Фреймворк**: PyTorch, DataLoader с `drop_last=True`, `num_workers=0`.  
- **Оптимизаторы**: Adam (β1=0.5) для DCGAN-части, RMSprop для WGAN.  
- **Hyper-tuning**:  
  - Популяция G — 5, отбор 2, σ_mut=0.02.  
  - Размер батча 128, эпох 50.  
- **Блокировка**: `plt.show()` заменено на сохранение через `save_image` для некоплицированного прогресса.

---

## 5. Выводы и рекомендации

1. **Градиент необходим** для обучения современных сверточных GAN — чистая эволюция без градиента не справляется.  
2. **Эволюция** (отбор + мутации) отлично дополняет градиент, поддерживая разнообразие и помогая выходить из локальных минимумов.  
3. Оптимальная конфигурация:  
   - Один дискриминатор, обучаемый традиционно.  
   - Небольшая популяция генераторов (5–10), каждый получает свой градиентный шаг + эволюционный отбор в конце эпохи.  
4. **Будущие направления**:  
   - Изучить soft-selection вместо жёсткого отбора,  
   - Добавить механизмы «кроссовера» параметров (скрещивание сетей),  
   - Автоматически адаптировать σ мутаций по сигналу сходимости,  
   - Оценить FID / IS для количественного сравнения.  

---

Эта работа демонстрирует, что **гибридный GAN**, сочетающий классический градиентный спуск и популяционные принципы Лотка–Вольтерра с мутациями, способен давать более устойчивое обучение по сравнению с чистым GAN или чистой эволюционной стратегией.
